\chapter{教師あり学習}

\section{トレーニングセットと仮説関数}

教師あり学習は，特徴量と目的変数の組のデータを用いて学習する．学習に使用するデータをトレーニングセットという．

\begin{defi}[トレーニングセット]
$\{(\bm{x}^{(i)},y^{(i)})\}_{i=1}^m \subset (({\cal X}_1 \times {\cal X}_2\times \cdots \times{\cal X}_n) \times {\cal Y})^m$を{\bf トレーニングセット(training set)}という．ここで，$m$は{\bf トレーニングサンプル数(number of training examples)}，${\bm x}^{(i)} \in {\cal X}_1 \times {\cal X}_2\times \cdots \times {\cal X}_n$は$i$番目の$n$個の{\bf 入力変数(input variables)}または{\bf 特徴量(features)}で，${\bm x}^{(i)}=(x_1^{(i)},x_2^{(i)},\ldots,x_n^{(i)})^T$と表す．また，$y^{(i)} \in {\cal Y}$は$i$番目の{\bf 出力変数(output variable)}または{\bf 目的変数(target variable)}である．また，トレーニングセットの$i$番目の要素$({\bm x}^{(i)},y^{(i)})\in ({\cal X}_1 \times {\cal X}_2\times \cdots \times{\cal X}_n) \times {\cal Y}$を{\bf トレーニングサンプル(training example)}という．また，入力変数のとる空間${\cal X}_1 \times {\cal X}_2\times \cdots \times{\cal X}_n$を{\bf 入力変数空間(space of input values)}，出力変数のとる空間${\cal Y}$を{\bf 出力変数空間(space of output values)}という．
\end{defi}

\begin{qu}
次のトレーニングセットにおいて，$x_3^{(4)},y^{(2)}$を答えよ．
\begin{table}[H]
\centering
\begin{tabular}{rrrrrr}
\hline
$i$ &$x_1^{(i)}$ &$x_2^{(i)}$ &$x_3^{(i)}$ &$x_4^{(i)}$ & $y^{(i)}$ \\ \hline
1 & 2104 & 5 & 1 & 45 & 460 \\
2 & 1416 & 3 & 2 & 40 & 232 \\
3 & 1534 & 3 & 2 & 30 & 315 \\
4 & 852  & 2 & 1 & 36 & 178 \\ \hline
\end{tabular}
\end{table}
\end{qu}
\begin{ans}
$x_3^{(4)}=1,~y^{(2)}=232$．\qed
\end{ans}

教師あり学習を使って解きたいタスク$T$は，入力変数から出力変数を予測することであるが，それは言い換えると入力変数を引数として出力変数を出力する写像を設定することである．この写像を仮説関数という．仮説関数を以下で定義する．

\begin{defi}[仮説関数]
入力変数${\bm x}$から出力変数$y$への写像$h:{\cal X}_1 \times {\cal X}_2\times \cdots \times{\cal X}_n \to {\cal Y}$，すなわち$h_{\bm \theta}({\bm x})$を{\bf 仮説関数(hypothesis function)}という．ここで，${\bm \theta}$は仮説関数の{\bf パラメータ}である(パラメータは複数あることがほとんどなのでベクトルとしている)．
\end{defi}

すなわち，教師あり学習とは，仮説関数を設定し，トレーニングセットを用いて仮説関数の最適なパラメータを決定することといえる．最適なパラメータを決定できれば，そのパラメータをセットした仮説関数にデータを流し込むことで，そのデータに対する予測値を計算できる．

\section{線形回帰}

仮説関数は自らで与える必要があるが，仮説関数の形によって，教師あり学習に特別な名前がつくものがある．例えば，仮説関数を線形関数とし，出力変数空間を${\cal Y}=\mathbb{R}$としたときは線形回帰という．

\begin{defi}[線形回帰]
トレーニングセット$\{({\bm x}^{(i)},y^{(i)})\}_{i=1}^m \subset (({\cal X}_1 \times {\cal X}_2\times \cdots \times{\cal X}_n) \times {\cal Y})^m$について，${\cal Y}=\mathbb{R}$であり，かつ仮説関数が式(\ref{LM})である教師あり学習を，特に{\bf 線形回帰(linear regression)}という．ここで，特徴量${\bm x}^{(i)}$は，常に1の値をとるような特徴量$x_0^{(i)}=1$を付して${\bm x}^{(i)}=(x_0^{(i)},x_1^{(i)},x_2^{(i)},\ldots,x_n^{(i)})^T \in 1 \times {\cal X}_1 \times {\cal X}_2\times \cdots \times{\cal X}_n$と置き直すこととする．また，${\bm \theta}=(\theta_0,\theta_1,\ldots,\theta_n)^T \in \mathbb{R}^{n+1}$とする．
\begin{align}
h_{{\bm \theta}}({\bm x}^{(i)}) &= \theta_0 x_0^{(i)}+\theta_1 x_1^{(i)} +\theta_2 x_2^{(i)}+\cdots + \theta_n x_n^{(i)} \nonumber \\
&= \sum_{j=0}^n \theta_j x_j^{(i)}\nonumber \\
&= {\bm \theta}^T {\bm x}^{(i)} \label{LM}
\end{align}
\end{defi}

\begin{qu}
ある大学生について，1年次の成績で優をとった個数から2年次にいくつ優をとるのか予測したい．そこで，何人かの大学生の1年次の成績の優の個数$x$と2年次の成績の優の個数$y$を集めた．その結果が次表である．このとき，次の問いに答えよ．
\begin{table}[H]
\centering
\begin{tabular}{rr}
\hline
$x$    & $y$   \\ \hline
3 & 4 \\
2 & 1 \\
4 & 3 \\
0  & 1 \\ \hline
\end{tabular}
\end{table}
\begin{enumerate}
\item $m$はいくつか．
\item 仮説関数として$h_{{\bm \theta}}(x)=\theta_0+\theta_1 x$を設定し，本トレーニングセットを用いて線形回帰を行なった結果，パラメータは$\theta_0=-1,~\theta_1=2$となった．このとき，1年次の優の個数が6だった大学生の2年次の優の個数を予測せよ．
\end{enumerate}
\end{qu}
\begin{ans}　
\begin{enumerate}
\item $m=4$．
\item $h_{{\bm \theta}}(x)=-1+2x$なので，$h_{{\bm \theta}}(6)=-1+2\cdot 6=11$．
\end{enumerate}
\qed	
\end{ans}

\subsection{目的関数}

さて，仮説関数のパラメータをどう決めるかという問題がある．パラメータをでたらめに与えても良い予測値を返さないので，経験$E$のトレーニングセットを使って，性能指標$P$を高めるように仮説関数のパラメータをアップデートしていくことが必要になる．この手順を学習アルゴリズムといい，性能指標を測る関数を目的関数という．

\begin{defi}[学習アルゴリズム，目的関数]
性能指標を測る関数$J({\bm \theta})$を{\bf 目的関数}または{\bf コスト関数(cost function)}といい，この目的関数で測った性能指標が良くなるように仮説関数(のパラメータ${\bm \theta}$)をアップデートしていく手順を{\bf 学習アルゴリズム(learning algorithm)}という．
\end{defi}

回帰問題における性能指標としては，各トレーニングサンプル$({\bm x}^{(i)},y^{(i)})$での仮説関数$h_{{\bm \theta}}({\bm x}^{(i)})$と$y^{(i)}$の二乗誤差平均が考えられる．この二乗誤差平均を計算する目的関数を最小二乗誤差関数という．

\begin{defi}[最小二乗誤差関数]
トレーニングセット$\{({\bm x}^{(i)},y^{(i)})\}_{i=1}^m \subset ((1 \times {\cal X}_1 \times {\cal X}_2\times \cdots \times{\cal X}_n) \times {\cal Y})^m$について，式(\ref{SEF})で表される目的関数$J({\bm \theta})$を{\bf 最小二乗誤差関数(squared error function)}という．
\begin{align}
J({\bm \theta}) = \frac{1}{2m}\sum_{i=1}^m (h_{{\bm \theta}}({\bm x}^{(i)})-y^{(i)})^2 \label{SEF}
\end{align}
\end{defi}

\begin{rem}
$m$ではなく$2m$で割っているのは，微分したときに出てくる2が消えるようにしているからである．$m$でも特段の問題はない(同じ結果が得られる)．
\end{rem}

\begin{qu}
あるトレーニングセット$\{(x^{(i)},y^{(i)})\}_{i=1}^3$をプロットしたところ以下の散布図となった(簡単のため，特徴量$x$に$x_0=1$である特徴量は追加していない）．仮説関数を$h_{\theta}(x)=\theta x$，目的関数$J(\theta)$を最小二乗誤差関数としたとき，$J(0)$を求めよ．
\vspace{-10pt}
\begin{figure}[H]
\begin{center}
\includegraphics[width=8.0cm]{code/lrfig1.eps}
\vspace{-10pt}
\caption{\texttt{lrfig1.eps}}
\end{center}
\end{figure}
\vspace{-25pt}
\end{qu}
\begin{ans}
目的関数$J(\theta)$は次式となる．
\begin{align*}
J(\theta)&=\frac{1}{6}\sum_{i=1}^3 (h_{\theta}(x^{(i)})-y^{(i)})^2 \\
&=\frac{1}{6}\sum_{i=1}^3 (\theta x^{(i)}-y^{(i)})^2
\end{align*}
$\theta =0$を代入し，散布図から$y^{(i)}$を読み取ると，
\begin{align*}
J(0)&=\frac{1}{6}\sum_{i=1}^3 (-y^{(i)})^2 \\
&=\frac{1}{6}((-1)^2+(-2)^2+(-3)^2) \\
&= \frac{14}{6}
\end{align*}
\qed
\end{ans}

\begin{cod}[\texttt{lrfig1.eps}作成プログラム(\texttt{lrfig1.py})]　
\lstinputlisting[backgroundcolor={\color[gray]{.95}}]{code/lrfig1.py}
\end{cod}

\subsection{最急降下法}

「性能指標が良くなるように仮説関数のパラメータ${\bm \theta}$をアップデートしていく」とはどういうことか．目的関数が最小二乗誤差関数の場合，その最小二乗誤差がどんどん小さくなっていくことが，性能指標が良くなっていくといえる．すなわち，目的関数を最小にするパラメータ${\bm \theta}$を見つければよい．それを見つけるための手法が学習アルゴリズムである．

学習アルゴリズムの中で一般的なものとして最急降下法がある．最急降下法とは，目的関数$J({\bm \theta})$のグラフ上に適当に点を打ち(すなわちパラメータ${\bm \theta}$として適当に初期値を決め），その点からあたりを見渡してもっとも勾配が急な方向に一定程度進み，進んだ後の点からまたあたりを見渡してもっとも勾配が急な方向に一定程度進み\ldots を繰り返して，どこを見渡しても勾配がないような点を探す方法である．

最急降下法を行うためには，関数上のある点について勾配が急な方向はどの方向であるかを計算しなければならない．勾配が最も急な方向を向くベクトルを勾配ベクトルといい，以下で定義される．
\begin{defi}[勾配ベクトル]
$k$次元ベクトル${\bm \theta}=(\theta_1,\theta_2,\ldots,\theta_k)^T$からスカラー値に写る関数$f({\bm \theta})$，すなわち$f:{\bm \theta} \in \mathbb{R}^k \to \mathbb{R}$である関数$f({\bm \theta})$において，{\bf 勾配ベクトル}$\nabla_{{\bm \theta}}f=\frac{\partial f}{\partial {\bm \theta}}$は次式で定義される．
\begin{align}
\nabla_{{\bm \theta}}f = \frac{\partial f}{\partial {\bm \theta}} = 
\begin{bmatrix}
\frac{\partial f}{\partial \theta_1} \\[5pt]
\frac{\partial f}{\partial \theta_2} \\[3pt]
\vdots \\[5pt]
\frac{\partial f}{\partial \theta_k}
\end{bmatrix}
\end{align}
\end{defi}

\begin{qu}
$f:\mathbb{R}^2 \to \mathbb{R}$の関数$f(x,y)$のある点P$(x,y)$における最大勾配方向が勾配ベクトルの方向と同方向であることを示せ．
\end{qu}
\begin{ans}
勾配とは，関数$f$の変化度合いであり，勾配が最大ということは，関数の変化度合いが最も大きいということである．点P$(x,y)$と，そこから微小量$\Delta x$，$\Delta y$だけ動かした点Q$(x+\Delta x,y+\Delta y)$においてそれぞれ関数値を求めて差をとったものを変化度合い$\Delta f$とすると，$\overrightarrow{PQ}=\overrightarrow{OQ}-\overrightarrow{OP}=(\Delta x,\Delta y)$に注意して，以下の通り変形できる．
\begin{align*}
\Delta f &= f(x+\Delta x,y+\Delta y)-f(x,y) \\
&= f(x+\Delta x,y+\Delta y) -f(x,y+\Delta y)+f(x,y+\Delta y) -f(x,y) \\
&= \frac{f(x+\Delta x,y+\Delta y) -f(x,y+\Delta y)}{\Delta x}\Delta x +\frac{f(x,y+\Delta y) -f(x,y)}{\Delta y}\Delta y \\
&\fallingdotseq \frac{\partial f}{\partial x}\Delta x +\frac{\partial f}{\partial y}\Delta y \\
&= \nabla f \cdot (\Delta x,\Delta y) \\
&= \nabla f \cdot \overrightarrow{PQ}
\end{align*}
すなわちこれは，関数の変化度合いは勾配ベクトルと点Pから点Qへの方向ベクトル，すなわち微小量を動かした方向のベクトルの内積となっている．ここで，角度の定義より，
\begin{align*}
\Delta f &= ||\nabla f||||\overrightarrow{PQ}||\cos \theta
\end{align*}
となる．ここで，$\theta$は，$\nabla f$と$\overrightarrow{PQ}$のなす角である．関数の変化度合い$\Delta f$が最大となるのは，$\cos \theta =1$，すなわち$\theta =0$となる場合である．これはつまり$\nabla f$と$\overrightarrow{PQ}$が同じ方向を向いているときに関数の変化度合い$\Delta f$が最大となるということである．以上より，点Pから関数の変化度合い$\Delta f$が最大となるように進むためには(点Qをとるためには)，勾配ベクトルの方向に進めばよいということである．\qed
\end{ans}

これで勾配が最も急な方向が勾配ベクトル方向であることがわかったので，それを用いて最急降下法を以下の通り定義する．
\begin{defi}[最急降下法]
関数$J({\bm \theta})$を最小とする${\bm \theta}$を次の手順で見つけるアルゴリズムを，{\bf 最急降下法(gradient descent algorithm)}という．ここで，$\alpha (>0)$を{\bf 学習率}といい，勾配が最大の方向にどの程度移動させるかの強さを表す．
\begin{algorithm}[H]
\caption{最急降下法}
\begin{algorithmic}[1]
\State{トレーニングセット$\{({\bm x}^{(i)},y^{(i)})\}_{i=1}^m$，仮説関数$h_{\bm \theta}({\bm x})$，目的関数$J({\bm \theta)}$を用意}
\State{$\alpha \gets 初期値$}
\State{${\bm \theta} \gets 初期値$}
\While{${\bm \theta}$が収束または有限回繰り返し}
\State{$\{({\bm x}^{(i)},y^{(i)})\}_{i=1}^m$を代入して$J({\bm \theta)}$を計算}
\State{$\nabla_{{\bm \theta}}J$を計算}

\State{${\bm \theta} \gets {\bm \theta}-\alpha \nabla_{{\bm \theta}}J$}\Comment{パラメータ$\theta_0,\theta_1\ldots $は同タイミングで更新}
\EndWhile
\end{algorithmic}
\end{algorithm}
\end{defi}

\begin{qu}
$x_0^{(i)}=1$も含め特徴量が$n+1$個のトレーニングセット$\{({\bm x}^{(i)},y^{(i)})\}_{i=1}^m$の線形回帰において，目的関数$J({\bm \theta})$を最小二乗誤差関数としたとき，$\nabla_{{\bm \theta}}J$を計算せよ．
\end{qu}
\begin{ans}
$\nabla_{{\bm \theta}}J$の$j$番目の要素$\frac{\partial J}{\partial \theta_j}$を計算すると以下となる．
\begin{align*}
\frac{\partial J}{\partial \theta_j}&= \frac{1}{2m}\sum_{i=1}^m \frac{\partial }{\partial \theta_j}(h_{{\bm \theta}}({\bm x}^{(i)})-y^{(i)})^2 \\
&=\frac{1}{2m}\sum_{i=1}^m 2(h_{{\bm \theta}}({\bm x}^{(i)})-y^{(i)})\frac{\partial }{\partial \theta_j}h_{{\bm \theta}}({\bm x}^{(i)})\\
&= \frac{1}{m}\sum_{i=1}^m (h_{{\bm \theta}}({\bm x}^{(i)})-y^{(i)})x_j^{(i)}
\end{align*}
$j=0$のときは$\frac{\partial }{\partial \theta_0}h_{{\bm \theta}}({\bm x}^{(i)})=1$であることに注意してまとめると，
\begin{align}
\nabla_{{\bm \theta}}J =\frac{1}{m}
\begin{bmatrix}
\sum_{i=1}^m (h_{{\bm \theta}}({\bm x}^{(i)})-y^{(i)}) \\[3pt]
\sum_{i=1}^m (h_{{\bm \theta}}({\bm x}^{(i)})-y^{(i)})x_1^{(i)} \\[3pt]
\sum_{i=1}^m (h_{{\bm \theta}}({\bm x}^{(i)})-y^{(i)})x_2^{(i)} \\[3pt]
\vdots \\[3pt]
\sum_{i=1}^m (h_{{\bm \theta}}({\bm x}^{(i)})-y^{(i)})x_j^{(i)} \\[3pt]
\vdots \\[3pt]
\sum_{i=1}^m (h_{{\bm \theta}}({\bm x}^{(i)})-y^{(i)})x_n^{(i)} \\[3pt]
\end{bmatrix}\label{simpleJ}
\end{align}
となる（$\nabla_{{\bm \theta}}J$は$n+1$次元ベクトルである）．\qed
\end{ans}

\subsection{デザイン行列}

最急降下法アルゴリズムは， 式(\ref{simpleJ})を用いて$\nabla_{{\bm \theta}}J$の各要素に対して逐次計算を行っていけば単純に実装できるが，各要素の和の計算を各要素に対して行い，反復していくことは少々ややこしい．行列計算を容易に行えるプログラミング言語で実装する場合には，できるだけ逐次計算をしないように，行列計算やベクトル計算を用いて工夫して実装することが簡潔かつバグも少ない．ここでは，デザイン行列を定義し，行列計算により$\nabla_{{\bm \theta}}J$を計算できることを示す．

\begin{defi}[デザイン行列]
$x_0^{(i)}=1$も含め特徴量が$n+1$個のトレーニングセット$\{({\bm x}^{(i)},y^{(i)})\}_{i=1}^m$において，式(\ref{designX})で定義する行列$X \in \mathbb{R}^{m\times (n+1)}$を{\bf デザイン行列(design matrix)}という．デザイン行列は，特徴量${\bm x}^{(i)}$を転置してサンプル数の分縦に並べたもので表される．ここで，特徴量として$x_0=1$が加わっていることに注意する．
\begin{align}
X = 
\begin{bmatrix}
\mbox{------} & {{\bm x}^{(1)}}^T & \mbox{------} \\
\mbox{------} & {{\bm x}^{(2)}}^T & \mbox{------} \\
 & \vdots & \\
\mbox{------} & {{\bm x}^{(m)}}^T & \mbox{------}
\end{bmatrix}
=
\begin{bmatrix}
1 & x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\
1 & x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & x_1^{(m)} & x_2^{(m)} & \cdots & x_n^{(m)} 
\end{bmatrix} \label{designX}
\end{align}
\end{defi}

\begin{qu}
次のトレーニングセットにおいて，デザイン行列$X$を答えよ．
\begin{table}[H]
\centering
\begin{tabular}{cccccc}
\hline
$i$ &$x_1^{(i)}$ &$x_2^{(i)}$ &$x_3^{(i)}$ &$x_4^{(i)}$ & $y^{(i)}$ \\ \hline
1 & 2104 & 5 & 1 & 45 & 460 \\
2 & 1416 & 3 & 2 & 40 & 232 \\
3 & 1534 & 3 & 2 & 30 & 315 \\
4 & 852  & 2 & 1 & 36 & 178 \\ \hline
\end{tabular}
\end{table}
\end{qu}
\begin{ans}
サンプル数が4つで，特徴量が$x_0=1$を含めると5つなので，$X$は$4\times 5$次元の行列となり，
\begin{align*}
X=
\begin{bmatrix}
1 & 2104 & 5 & 1 & 45 \\
1 & 1416 & 3 & 2 & 40 \\
1 & 1534 & 3 & 2 & 30 \\
1 & 852  & 2 & 1 & 36 
\end{bmatrix}
\end{align*}\qed
\end{ans}

デザイン行列を使うと，予測値である線形回帰の仮説関数のベクトルを簡潔に表すことができ，1発の線形代数計算で予測値を計算できる．

\begin{qu}
$x_0^{(i)}=1$も含め特徴量が$n+1$個のトレーニングセット$\{({\bm x}^{(i)},y^{(i)})\}_{i=1}^m$における線形回帰問題を考える．このとき，$(h_{{\bm \theta}}({\bm x}^{(1)}),h_{{\bm \theta}}({\bm x}^{(2)}),\ldots,h_{{\bm \theta}}({\bm x}^{(m)}))^T$を$X$，${\bm \theta}$を用いて表せ．
\end{qu}
\begin{ans}
式(\ref{tenkai1})より，以下となる．
\begin{align}
\begin{bmatrix}
h_{{\bm \theta}}({\bm x}^{(1)}) \\
h_{{\bm \theta}}({\bm x}^{(2)}) \\
\vdots \\
h_{{\bm \theta}}({\bm x}^{(m)})
\end{bmatrix}
=
\begin{bmatrix}
{\bm \theta}^T{\bm x}^{(1)} \\
{\bm \theta}^T {\bm x}^{(2)}\\
\vdots \\
{\bm \theta}^T {\bm x}^{(m)}
\end{bmatrix}
=
\begin{bmatrix}
{{\bm x}^{(1)}}^T{\bm \theta} \\
{{\bm x}^{(2)}}^T{\bm \theta} \\
\vdots \\
{{\bm x}^{(m)}}^T{\bm \theta}
\end{bmatrix}
=
\begin{bmatrix}
\mbox{------} & {{\bm x}^{(1)}}^T & \mbox{------} \\
\mbox{------} & {{\bm x}^{(2)}}^T & \mbox{------} \\
 & \vdots & \\
\mbox{------} & {{\bm x}^{(m)}}^T & \mbox{------}
\end{bmatrix}
\begin{bmatrix}
| \\[-2pt]
| \\
{\bm \theta} \\
| \\[-2pt]
|
\end{bmatrix}
=X {\bm \theta} \label{lr_hypo}
\end{align}
\qed
\end{ans}

この結果を用いることで，目的関数を最小二乗誤差関数とした線形回帰問題において，目的関数をより簡潔に表すことができる．

\begin{qu}
$x_0^{(i)}=1$も含め特徴量が$n+1$個のトレーニングセット$\{({\bm x}^{(i)},y^{(i)})\}_{i=1}^m$における線形回帰問題において，目的関数$J({\bm \theta})$を最小二乗誤差関数とする．このとき，$J({\bm \theta})$を$X$，${\bm \theta}$，${\bm y}$を用いて表せ．ここで，${\bm y}=(y^{(1)},y^{(2)},\ldots,y^{(m)})^T$とする．
\end{qu}
\begin{ans}
\begin{align}
J({\bm \theta})&= \frac{1}{2m}\sum_{i=1}^m (h_{{\bm \theta}}({\bm x}^{(i)})-y^{(i)})^2 \nonumber \\
&=\frac{1}{2m}
\begin{bmatrix}
h_{{\bm \theta}}({\bm x}^{(1)})-y^{(1)} & h_{{\bm \theta}}({\bm x}^{(2)})-y^{(2)} & \cdots & h_{{\bm \theta}}({\bm x}^{(m)})-y^{(m)}
\end{bmatrix}
\begin{bmatrix}
h_{{\bm \theta}}({\bm x}^{(1)})-y^{(1)} \\
h_{{\bm \theta}}({\bm x}^{(2)})-y^{(2)} \\
\vdots \\
h_{{\bm \theta}}({\bm x}^{(m)})-y^{(m)}
\end{bmatrix}\nonumber
\end{align}
となるが，ここで，
\begin{align}
\begin{bmatrix}
h_{{\bm \theta}}({\bm x}^{(1)})-y^{(1)} \\
h_{{\bm \theta}}({\bm x}^{(2)})-y^{(2)} \\
\vdots \\
h_{{\bm \theta}}({\bm x}^{(m)})-y^{(m)}
\end{bmatrix}
=
\begin{bmatrix}
h_{{\bm \theta}}({\bm x}^{(1)}) \\
h_{{\bm \theta}}({\bm x}^{(2)}) \\
\vdots \\
h_{{\bm \theta}}({\bm x}^{(m)})
\end{bmatrix}
-
\begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(m)}
\end{bmatrix}
=X{\bm \theta}-{\bm y} \nonumber
\end{align}
より，
\begin{align}
J({\bm \theta})&= \frac{1}{2m}(X{\bm \theta}-{\bm y})^T(X{\bm \theta}-{\bm y}) \label{lr_cost}
\end{align}\qed
\end{ans}

これで目的関数を簡潔に書けたので，最後にそれを微分して$\nabla_{{\bm \theta}}J$を求める．ここで，線形形式，二次形式の勾配ベクトルを使用するので事前に述べておく．
\begin{theo}[線形形式，二次形式の勾配ベクトル]
$n$次元列ベクトル${\bm x}，{\bm a}$，$n\times n$行列$A$に対して，次式が成り立つ．
\begin{align}
\nabla_{{\bm x}}({\bm a}^T {\bm x})&=\nabla_{{\bm x}}({\bm x}^T {\bm a})={\bm a}\\
\nabla_{{\bm x}}({\bm x}^T A{\bm x})&=(A+A^T){\bm x}
\end{align}
\end{theo}
\begin{pro}
線形形式の勾配ベクトルの証明は略．二次形式の勾配ベクトルは式(\ref{quadform})から計算すると楽である．式(\ref{quadform})において，$x_i$における偏微分を計算すると以下となる．変形の最後は，$A=\{a_{ij}\}$の転置行列を$A^T=\{a'_{ij}\}$としたとき，$a_{ki}=a'_{ik}$であることを使用している．
\begin{align*}
\frac{\partial }{\partial x_i}({\bm x}^T A{\bm x})&=\frac{\partial }{\partial x_i}\left(a_{ii}x_i^2+\sum_{\substack{j=1\\j\neq i}}^n a_{ij}x_ix_j+\sum_{\substack{k=1\\k\neq i}}^n a_{ki}x_kx_i+\sum_{\substack{j,k\\j\neq i\\k\neq i}}a_{kj}x_kx_j\right) 	\\
&=2a_{ii}x_i+\sum_{\substack{j=1\\j\neq i}}^n a_{ij}x_j+\sum_{\substack{k=1\\k\neq i}}^n a_{ki}x_k\\
&=\sum_{j=1}^na_{ij}x_j+\sum_{k=1}^na_{ki}x_k\\
&=\sum_{j=1}^na_{ij}x_j+\sum_{k=1}^na'_{ik}x_k
\end{align*}
第1項について，$x_1〜x_n$についての結果を列ベクトルとして並べると，式(\ref{tenkai2})の形が現れ，その結果は$A{\bm x}$となる．第2項についても同様であり，その結果は$A^T{\bm x}$となる．以上より，$\nabla_{{\bm x}}({\bm x}^T A{\bm x})=A{\bm x}+A^T{\bm x}=(A+A^T){\bm x}$である．
\qed	
\end{pro}

\begin{qu}
$x_0^{(i)}=1$も含め特徴量が$n+1$個のトレーニングセット$\{({\bm x}^{(i)},y^{(i)})\}_{i=1}^m$における線形回帰問題において，目的関数$J({\bm \theta})$を最小二乗誤差関数とする．このとき，$\nabla_{{\bm \theta}}J$を$m$，$X$，${\bm \theta}$，${\bm y}$で表せ．
\end{qu}
\begin{ans}
$X^TX$は対称行列であることに注意すると，
\begin{align}
\nabla_{{\bm \theta}}J&=\nabla_{{\bm \theta}}\left(\frac{1}{2m}(X{\bm \theta}-{\bm y})^T(X{\bm \theta}-{\bm y})\right) \nonumber\\
&=\nabla_{{\bm \theta}}\left(\frac{1}{2m}\left((X{\bm \theta})^T(X{\bm \theta})-(X{\bm \theta})^T{\bm y}-{\bm y}^T(X{\bm \theta})+{\bm y}^T{\bm y}\right)\right)\nonumber\\
&=\nabla_{{\bm \theta}}\left(\frac{1}{2m}\left((X{\bm \theta})^T(X{\bm \theta})-2(X{\bm \theta})^T{\bm y}+{\bm y}^T{\bm y}\right)\right)\nonumber\\
&=\nabla_{{\bm \theta}}\left(\frac{1}{2m}\left( {\bm \theta}^TX^TX{\bm \theta}-2{\bm \theta}^TX^T{\bm y}+{\bm y}^T{\bm y} \right)\right) \nonumber\\
&=\frac{1}{2m}\left(\nabla_{{\bm \theta}}({\bm \theta}^TX^TX{\bm \theta})-2\nabla_{{\bm \theta}}({\bm \theta}^TX^T{\bm y})+\nabla_{{\bm \theta}}({\bm y}^T{\bm y})\right)\nonumber\\
&=\frac{1}{2m}\left(2X^TX{\bm \theta}-2X^T{\bm y}\right)\nonumber \\
&=\frac{1}{m}\left(X^TX{\bm \theta}-X^T{\bm y}\right) \label{LRglad}
\end{align}\qed
\end{ans}

\subsection{Pythonによる実装}

以上で線形回帰で必要なパーツは揃った．これらのパーツを実際にPythonにて実装していきながら，線形回帰とはどういうものかを掴んでいく．

\begin{qu}\label{qu_lr5}
\texttt{ex1data1.txt}(2列，カンマ区切り，列名ヘッダーなし，1列目はpopulation of city in 10,000s，2列目はprofit in \$10,000sであるデータ．取得元は\cite{AndrewML})を散布図でプロットし，すなわち仮説関数を$h_{{\bm \theta}}({\bm x}^{(i)})={\bm \theta}^T{\bm x}^{(i)}$とする線形回帰を行うことが良さそうなことを確認せよ．
\end{qu}
\begin{ans}
\begin{cod}[\texttt{lr5.py}]　
\lstinputlisting[backgroundcolor={\color[gray]{.95}}]{code/lr5.py}
\vspace{-19pt}
\begin{figure}[H]
\begin{center}
\framed
\includegraphics[width=8.0cm]{code/lrfig2.eps}
\vspace{-10pt}
\caption{\texttt{lrfig2.eps}}
\endframed
\end{center}
\end{figure}
%\begin{lstlisting}
%\end{lstlisting}
\end{cod}
\vspace{-20pt}
\end{ans}


\begin{qu}\label{qu_lr1}
(問題\ref{qu_lr5}の続き) \texttt{ex1data1.txt}から，線形回帰を行うための準備としてデザイン行列$X$と目的変数${\bm y}$をそれぞれ\texttt{<2d\_ndarray>}で作成せよ．ここで，目的変数はprofit in \$10,000sとし，デザイン行列には$x_0^{(i)}=1$の項も付け加えよ．
\end{qu}
\begin{ans}
表形式データは，行方向にデータが並び，列方向が特徴量が並ぶものが多いので，デザイン行列は表形式データそのものとして与えてあげればよい．また，目的変数は1列のデータであるが，\texttt{<DataFrame>}から1列だけ読み込む場合は\texttt{<1d\_ndarray>}になってしまうので，列ベクトルとしての\texttt{<2d\_ndarray>}に変換する．なお，データ数が多いので，以下のコードにおいてはデザイン行列や目的変数は最初の5行分のみ表示するようにしている．
\begin{cod}[\texttt{lr1.py}]　
\lstinputlisting[backgroundcolor={\color[gray]{.95}}]{code/lr1.py}
\vspace{-10pt}
\begin{lstlisting}
X=
[[1.     6.1101]
 [1.     5.5277]
 [1.     8.5186]
 [1.     7.0032]
 [1.     5.8598]]
......
type=<class 'numpy.ndarray'>,shape=(97, 2)
y=
[[17.592 ]
 [ 9.1302]
 [13.662 ]
 [11.854 ]
 [ 6.8233]]
......
type=<class 'numpy.ndarray'>,shape=(97, 1)
\end{lstlisting}
\end{cod}
\vspace{-10pt}
\qed
\end{ans}

\begin{qu}\label{qu_lr2}
(問題\ref{qu_lr1}の続き) 何らかの学習を行なった結果，仮説関数のパラメータとして$\bm{\theta}=(1,-2)^T$を得られた．このとき，問題\ref{qu_lr1}で作成したデザイン行列$X$に対して予測値である仮説関数ベクトル$(h_{{\bm \theta}}({\bm x}^{(1)}),h_{{\bm \theta}}({\bm x}^{(2)}),\ldots,h_{{\bm \theta}}({\bm x}^{(m)}))^T$を計算し，問題\ref{qu_lr1}で作成した目的変数${\bm y}$と並べて表示することで，予測値の妥当性を確認せよ．なお，仮説関数は関数として定義せよ．
\end{qu}
\begin{ans}
式(\ref{lr_hypo})を実装すれば良い．
\begin{cod}[\texttt{lr2.py}]　
\lstinputlisting[backgroundcolor={\color[gray]{.95}}]{code/lr2.py}
\vspace{-10pt}
\begin{lstlisting}
y_df=
         y   y_pred
0  17.5920 -11.2202
1   9.1302 -10.0554
2  13.6620 -16.0372
3  11.8540 -13.0064
4   6.8233 -10.7196
\end{lstlisting}
\end{cod}
\vspace{-10pt}
予測結果より，$\bm{\theta}=(1,-2)^T$では全然予測になっていないことがわかる．
\qed
\end{ans}

\begin{qu}\label{qu_lr3}
(問題\ref{qu_lr2}の続き) 仮説関数のパラメータを$\bm{\theta}=(1,-2)^T$とする．このとき，問題\ref{qu_lr1}で作成したデザイン行列$X$と目的変数${\bm y}$に対して目的関数$J({\bm \theta})$の値を計算せよ．ここで，目的関数$J({\bm \theta})$は最小二乗誤差関数とする．なお，実装にあたっては，目的関数$J({\bm \theta})$は関数として定義せよ．
\end{qu}
\begin{ans}
式(\ref{lr_cost})を実装すれば良い．numpyでの線形代数計算は，結果がスカラーだとしても\texttt{<2d\_ndarray>}のままなので，要素を指定してあげて数値を取り出すことが必要となる．
\begin{cod}[\texttt{lr3.py}]　
\lstinputlisting[backgroundcolor={\color[gray]{.95}}]{code/lr3.py}
\vspace{-10pt}
\begin{lstlisting}
J=303.8795839611464
\end{lstlisting}
\end{cod}
\vspace{-10pt}
目的関数の数値の大きさからも，${\bm \theta}$は最適化できていないことがわかる．
\qed
\end{ans}

\begin{qu}\label{qu_lr6}
(問題\ref{qu_lr3}の続き) 仮説関数のパラメータを$\bm{\theta}=(1,-2)^T$とする．このとき，\texttt{ex1data1.txt}の散布図と，問題\ref{qu_lr2}で計算した予測値を線でプロットせよ．
\end{qu}
\begin{ans}
\begin{cod}[\texttt{lr6.py}]　
\lstinputlisting[backgroundcolor={\color[gray]{.95}}]{code/lr6.py}
\vspace{-19pt}
\begin{figure}[H]
\begin{center}
\framed
\includegraphics[width=8.0cm]{code/lrfig3.eps}
\vspace{-10pt}
\caption{\texttt{lrfig3.eps}}
\endframed
\end{center}
\end{figure}
%\begin{lstlisting}
%\end{lstlisting}
\end{cod}
\vspace{-20pt}
図示すると予測が全然できていないことが一目瞭然．
\qed	
\end{ans}

\begin{qu}\label{qu_lr4}
(問題\ref{qu_lr6}の続き) \texttt{ex1data1.txt}のデータを全て用いて目的変数${\bm y}$を特徴量から予測するモデルを線形回帰で構築するとしたとき，次の問いに答えよ．
\begin{enumerate}
\item 線形回帰の予測値である仮説関数$h_{{\bm \theta}}({\bm x})$のパラメータ${\bm \theta}$を求めよ．ここで，目的関数$J({\bm \theta})$は最小二乗誤差関数とし，${\bm \theta}$を求めるときに使用するアルゴリズムは最急降下法とする．また，最急降下法の学習率$\alpha $は$\alpha = 0.01$，${\bm \theta}$の初期値は${\bm \theta}=(1,-2)^T$とし，収束判定は行わず1500回パラメータの更新を行ったらアルゴリズムは終了するように実装せよ．
\item 求めた${\bm \theta}$に対する目的関数$J({\bm \theta})$の値を求めよ．
\item 新しくpopulation of city in 10,000sのデータ$x=7.532$を得たとする．この$x$に対してprofit in \$10,000sがいくらになるか，上で構築した線形回帰モデルを用いて予測せよ．
\end{enumerate}
\end{qu}
\begin{ans}
最急降下法における$J({\bm \theta})$の勾配ベクトルは式(\ref{LRglad})を実装すればよい．その他，仮説関数や目的関数は問題\ref{qu_lr2}，問題\ref{qu_lr3}で実装したものをベースに組めばよい．
\begin{cod}[\texttt{lr4.py}]　
\lstinputlisting[backgroundcolor={\color[gray]{.95}}]{code/lr4.py}
\vspace{-10pt}
\begin{lstlisting}
1: theta=
[[-3.55089376]
 [ 1.15838599]]
2: J=4.4878002526614615
3: y_pred=5.174069533566031
\end{lstlisting}
\end{cod}
\vspace{-10pt}
${\bm \theta}=(1,-2)^T$の場合の$J({\bm \theta})$の値からかなり減少していることがわかる．
\qed
\end{ans}

\begin{qu}\label{qu_lr7}
(問題\ref{qu_lr4}の続き) \texttt{ex1data1.txt}の散布図と，問題\ref{qu_lr4}で最適化したパラメータ${\bm \theta}$で\texttt{ex1data1.txt}のデータに対する線形回帰の予測値を線でプロットせよ．
\end{qu}
\begin{ans}
\begin{cod}[\texttt{lr7.py}]　
\lstinputlisting[backgroundcolor={\color[gray]{.95}}]{code/lr7.py}
\vspace{-19pt}
\begin{figure}[H]
\begin{center}
\framed
\includegraphics[width=8.0cm]{code/lrfig4.eps}
\vspace{-10pt}
\caption{\texttt{lrfig4.eps}}
\endframed
\end{center}
\end{figure}
%\begin{lstlisting}
%\end{lstlisting}
\end{cod}
\vspace{-20pt}
データに対しては良さそうな予測ができていることが確認できる(未知のデータに対しても良い予測かどうかは分からないことに注意)．
\qed	
\end{ans}

\begin{qu}\label{qu_lr8}
(問題\ref{qu_lr7}の続き) 目的関数$J({\bm \theta})$について，3次元の曲面図と2次元の等高線図を図示し，等高線図について最適化したパラメータ${\bm \theta}$をプロットして最適化ができていること(最小値付近にいること)を確認せよ．なお，図示の際の定義域は，$-15 \leq \theta_0 \leq 10,~-1 \leq \theta_1 \leq 4$とし，等高線図の図示の際は等高線ができるだけ等間隔に並ぶように\texttt{levels}を適切に設定せよ．
\end{qu}
\begin{ans}
\begin{cod}[\texttt{lr8.py}]　
\lstinputlisting[backgroundcolor={\color[gray]{.95}}]{code/lr8.py}
\vspace{-19pt}
\begin{figure}[H]
\begin{center}
\framed
\includegraphics[width=13.0cm]{code/lrfig5.eps}
\vspace{-10pt}
\caption{\texttt{lrfig5.eps}}
\endframed
\end{center}
\end{figure}
%\begin{lstlisting}
%\end{lstlisting}
\end{cod}
確かに最小値っぽいところに落ち着いているので，最急降下法はうまく動いてくれたようだ．
\qed	
\end{ans}

\begin{qu}\label{qu_lr9}
(問題\ref{qu_lr8}の続き) 最急降下法がうまくいっていることを確認するため，繰り返し計算が進むにつれて${\bm \theta}$が最小値付近に移動することや，仮説関数がデータをよく予測できていく様子を描画したい．最急降下法の${\bm \theta}$の更新ごとに${\bm \theta}$をストックするようなプログラムを作り，それを使用して繰り返し回数が0,1,10,1000回それぞれにおける仮説関数と$J({\bm \theta})$の等高線図における${\bm \theta}$のプロットを描画せよ．なお，最急降下法の初期値は${\bm \theta}=(0,0)^T$とすること．
\end{qu}
\begin{ans}
\begin{cod}[\texttt{lr9.py}]　
\lstinputlisting[backgroundcolor={\color[gray]{.95}}]{code/lr9.py}
\vspace{-19pt}
\begin{figure}[H]
\begin{center}
\framed
\includegraphics[width=13.5cm]{code/lrfig6.eps}
\vspace{-10pt}
\caption{\texttt{lrfig6.eps}}
\endframed
\end{center}
\end{figure}
%\begin{lstlisting}
%\end{lstlisting}
\end{cod}
最初の1回の更新でかなり学習が進んでいることがわかる．
\qed	
\end{ans}

\subsection{デバッグ}

さて，これまで最急降下法の学習率は$\alpha=0.01$固定でずっと行ってきた．この学習率を別の数値にした場合，最急降下法はどういう挙動を示すのか知りたいとする．最急降下法は目的関数$J({\bm \theta})$の数値をどんどん小さくするためのアルゴリズムであるから，最急降下法がうまく収束しているかを確認する手段としては，ループ1回ごとに$J({\bm \theta})$の値をプロットしていき，値が順調に減少していっているかどうかをみるという方法がある．これをデバッグという．

\begin{defi}[デバッグ]
最急降下法において，横軸に繰り返し回数(number of iterations)，縦軸に目的関数値$J({\bm \theta})$をとり図示することを{\bf デバッグ(debugging)}という．
\end{defi}

\begin{qu}\label{qu_lr10}
(問題\ref{qu_lr9}の続き) 学習率$\alpha=0.00001,0.0001,0.024324$の3パターンそれぞれで最急降下法のデバッグを行い，目的関数$J({\bm \theta})$の変化具合を図示することで比較し，最も適している学習率は3つのうちどれか答えなさい．なお，最急降下法の初期値は${\bm \theta}=(1,-2)^T$とすること．
\end{qu}
\begin{ans}
\begin{cod}[\texttt{lr10.py}]　
\lstinputlisting[backgroundcolor={\color[gray]{.95}}]{code/lr10.py}
\vspace{-19pt}
\begin{figure}[H]
\begin{center}
\framed
\includegraphics[width=15cm]{code/lrfig7.eps}
\vspace{-10pt}
\caption{\texttt{lrfig7.eps}}
\endframed
\end{center}
\end{figure}
%\begin{lstlisting}
%\end{lstlisting}
\end{cod}
最も適している学習率は，図より$\alpha=0.0001$である．学習率とは，パラメータ更新の強さを表すので，弱いとなかなか最小にたどり着かず，かといって強すぎると最小値を超えてパラメータを更新してしまうので，いつまでたっても最小にはたどり着かず，むしろどんどん遠ざかることなる．
\qed	
\end{ans}

\subsection{正規方程式}

最急降下法で収束して得られた${\bm \theta}$は，目的関数$J({\bm \theta})$の最小値である点であるためどこを見渡しても勾配がない状態となっている．これは，言い換えると目的関数$J({\bm \theta})$の勾配ベクトルがゼロベクトルとなる点${\bm \theta}$は，方程式$\nabla_{{\bm \theta}}J={\bm 0}$の解であることと等しい．この方程式は正規方程式といわれる．
\begin{defi}[正規方程式]
目的関数$J({\bm \theta})$の回帰問題について，次式を{\bf 正規方程式(normal equation formula)}という．
\begin{align}
\nabla_{{\bm \theta}}J={\bm 0}
\end{align}
\end{defi}

\begin{qu}
特徴量が$n$であるトレーニングセット$\{({\bm x}^{(i)},y^{(i)})\}_{i=1}^m$における線形回帰問題において，目的関数$J({\bm \theta})$を最小二乗誤差関数とする．このとき，$\nabla_{{\bm \theta}}J={\bm 0}$を簡単に表せ．
\end{qu}
\begin{ans}
式(\ref{LRglad})より，
\begin{align}
X^TX{\bm \theta}=X^T{\bm y}	
\end{align}\qed
\end{ans}
これを${\bm \theta}$について解けば最適なパラメータ${\bm \theta}$を得ることができる．ここで，$X^TX$が正則であれば，逆行列$(X^TX)^{-1}$が存在するのでそれを左から掛けることによって解けるが，正則でない場合（非正則，非可逆，特異の場合ともいう），逆行列を持たず，正規方程式は解を持たない（不能）もしくは複数または無数の解（不定）となる．不能や不定だからそれでお手上げ，というわけにはいかないので，「いい感じの」の解を設定したいとする．この「いい感じ」の解は，$X^TX$の{\bf ムーア・ペンローズ一般逆行列}$(X^TX)^+$を用いて，次式で書ける．
\begin{align}
{\bm \theta}=(X^TX)^+ X^T{\bm y} \label{lr_NEF}
\end{align}
この議論の詳細は，まだ著者が理解できていないため，今の所は割愛する．

\begin{qu}\label{qu_lr11}
(問題\ref{qu_lr10}の続き) 正規方程式を解いて最適なパラメータ${\bm \theta}$を求め，そのパラメータでの予測値と，最急降下法による予測値を図示することによって比較せよ．
\end{qu}
\begin{ans}
式(\ref{lr_NEF})を実装すればよい．
\begin{cod}[\texttt{lr11.py}]　
\lstinputlisting[backgroundcolor={\color[gray]{.95}}]{code/lr11.py}
\vspace{-19pt}
\begin{figure}[H]
\begin{center}
\framed
\includegraphics[width=8cm]{code/lrfig8.eps}
\vspace{-10pt}
\caption{\texttt{lrfig8.eps}}
\endframed
\end{center}
\end{figure}
%\begin{lstlisting}
%\end{lstlisting}
\end{cod}

\qed
\end{ans}

では，どのような場合に$X^TX$は非正則なのか．ここは著者がまだ理解できていないが，\cite{AndrewML}によれば，以下2つの場合を念頭に置いておけばとりあえず良いとのこと．
\begin{enumerate}
\item {\bf 特徴量が冗長}：例えば，住宅価格の予測についての特徴量で，縦の長さ，横の長さ，面積の3つを考えた場合，面積は縦の長さと横の長さですでに捉えられているので，面積という特徴量が冗長である．
\item {\bf データ数より特徴量が多い}($m \leq n$)：$n=100$個の特徴量を，$m=10$サンプルでフィッティングするのは，うまくいくこともあるかもしれないが良いアイデアではない．
\end{enumerate}

教師あり学習の回帰問題について，パラメータを探す方法として最急降下法と正規方程式を解く方法の2種類を取り上げた．それぞれの手法のメリットやデメリットについて，\cite{AndrewML}によれば以下の通り．
\begin{itemize}
\item 最急降下法は，学習率$\alpha $を適切に選択する必要があるため，良さげな数値を何度か試行することが必要となってくる場合がある．一方で，正規方程式はその手間がない．
\item 最急降下法の場合は，アルゴリズムがちゃんと機能しているか，ちゃんと収束しているかなどを確認しなければならないが，正規方程式はその手間がない．
\item 正規方程式の場合，特徴量スケーリングを行う必要はない．
\item 正規方程式は，逆行列を求める必要があるが，逆行列を計算するコストが非常に大きい．具体的には，逆行列の計算コストは特徴量$n$の3乗のオーダーとなる．一方，最急降下法は特徴量$n$本に対する計算を繰り返すだけなので，計算コストは$n$のオーダーと非常に少ない．つまり，最急降下法は特徴量が数百万個あるような場合でも正しく機能する．だいたい$n=10000$が正規方程式ではなく最急降下法を選ぶ目安．
\end{itemize}

\subsection{特徴量スケーリング}

これまでの実例では，特徴量は1つの場合しか取り扱ってこなかった．しかし，特徴量は複数あることの方が普通である．複数であっても，上記の実例と同様に線形代数計算と最急降下法で同じように実装できる．ここで，最急降下法を適用するにあたり，複数の特徴量がある場合，各特徴量のとりうる範囲がだいたい同じような範囲にあると，収束速度は速くなる．特徴量を変換してとりうる範囲の調整を行うことを，特徴量スケーリングという．

\begin{defi}[特徴量スケーリング]
トレーニングセット$\{({\bm x}^{(i)},y^{(i)})\}_{i=1}^m$について，${\bm x}^{(i)}$の各特徴量のとりうる範囲を同じような範囲に変換することを{\bf 特徴量スケーリング(feature scalling)}という．特に，特徴量$x_j$の平均$\mu_j$と標準偏差$s_j$を用いて$\frac{x_j-\mu_j}{s_j}$のようにスケーリングすることを，{\bf 平均標準化(mean normalization)}という．
\end{defi}

\begin{rem}
特徴量スケーリングを行うと${\bm x}^{(i)}\to {\bm x}'^{(i)}$と変換されるが，その場合は変換後のトレーニングセット$\{({\bm x}'^{(i)},y^{(i)})\}_{i=1}^m$に対して線形回帰を行う(パラメータ${\bm \theta}$を定める)ことになるため，$y^{(i)}$の予測値である仮説関数は$h_{{\bm \theta}}({\bm x}^{(i)})={\bm \theta}^T{\bm x}^{(i)}$ではなく，$h_{{\bm \theta}}({\bm x}'^{(i)})={\bm \theta}^T{\bm x}'^{(i)}$の形となる．よって，トレーニングセットにはない新しいデータ${\bm x}^{(m+1)}$が得られた場合，その予測値を計算するためには新しいデータに対しても特徴量スケーリングを施し${\bm x}'^{(m+1)}$と変換した上で仮説関数$h_{{\bm \theta}}({\bm x}'^{(m+1)})={\bm \theta}^T{\bm x}'^{(m+1)}$を計算し予測することになる．よって，特徴量スケーリングに用いた平均値や標準偏差等の数値や，その変換方法は学習が終わっても保持しておく必要がある．
\end{rem}

\begin{qu}\label{qu_lr2_1}
データ\texttt{ex1data2.txt}は，オレゴン州ポートランドの住宅価格のデータ(最初の列は住宅の面積(単位はsquare feet)，2番目の列は寝室の数，3番目の列は住宅価格)である．\texttt{ex1data2.txt}を用いて，オレゴン州ポートランドの住宅価格の予測モデルを線形回帰を用いて構築したい．このとき，次の問いに答えなさい．
\begin{enumerate}
\item データ\texttt{ex1data2.txt}をそのまま使用して学習し，最適なパラメータ${\bm \theta}$を求め，新しいデータ${\bm x}^{(m+1)}:住宅面積=2500, 寝室の数=3$に対する住宅価格の予測値を求めよ．ここで，学習率はデバッグをしながら自身で定めよ．また，繰り返し回数は50回で打ち切り，${\bm \theta}={\bm 0}$を初期値とする．
\item データ\texttt{ex1data2.txt}に特徴量スケーリングを施して学習し，最適なパラメータ${\bm \theta}$を求め，新しいデータ${\bm x}^{(m+1)}:住宅面積=2500, 寝室の数=3$に対する住宅価格の予測値を求めよ．ここで，学習率はデバッグをしながら自身で定めよ．また，繰り返し回数は50回で打ち切り，${\bm \theta}={\bm 0}$を初期値とする．また，特徴量スケーリングは$x_0=1$の列は施さなくてよい．
\item 特徴量スケーリングによりどう変わったのか確認せよ．
\end{enumerate}
\end{qu}
\begin{ans}　
\begin{enumerate}
\item これまで組んできたプログラムは特徴量が2つ以上の場合でも問題なく動くように書いてきたので，特段新しいことは必要としない．$\alpha $を変えながらひらすらデバッグをしていい感じに収束していくようなパターンを見つけていく．その結果，$\alpha = 0.00000003$の場合がいい感じであった．
\end{enumerate}
\begin{cod}[\texttt{lr12.py}]　
\lstinputlisting[backgroundcolor={\color[gray]{.95}}]{code/lr12.py}
\vspace{-10pt}
\begin{lstlisting}
y_pred=413220.18984796316
\end{lstlisting}
\begin{figure}[H]
\begin{center}
\framed
\includegraphics[width=8cm]{code/lrfig9.eps}
\vspace{-10pt}
\caption{\texttt{lrfig9.eps}}
\endframed
\end{center}
\end{figure}
\end{cod}
\vspace{-25pt}
\begin{enumerate}
\item[2.] 特徴量スケーリングを行う関数は，スケーリングに使用する平均値や標準偏差も併せて返すようにし，新しいデータに対する特徴量スケーリングを行う関数の引数として使えるようにする．あとは，$\alpha $を変えながらひらすらデバッグをしていい感じに収束していくようなパターンを見つけていく．その結果，$\alpha = 0.3$の場合がいい感じであった．
\end{enumerate}
\begin{cod}[\texttt{lr13.py}]　
\lstinputlisting[backgroundcolor={\color[gray]{.95}}]{code/lr13.py}
\vspace{-10pt}
\begin{lstlisting}
y_pred=411368.42281033465
\end{lstlisting}
\begin{figure}[H]
\begin{center}
\framed
\includegraphics[width=8cm]{code/lrfig10.eps}
\vspace{-10pt}
\caption{\texttt{lrfig10.eps}}
\endframed
\end{center}
\end{figure}
\end{cod}
\begin{enumerate}
\item[3.] 特徴量スケーリングをすると学習率はありうる自然な値とすることができる程度で，学習率として適切なものを設定できれば特徴量スケーリングをしてもしなくても少ない繰り返し回数で収束させることができるのではと考えられる．とはいえ，探索する学習率の範囲が広大だとデバッグに苦労するため，範囲を狭くするという意味で特徴量スケーリングを施す意義はあると思われる．なお，特徴量スケーリングはあくまでも学習効率を上げるための手法であり，予測精度を高める手法ではないことに注意する．実際，上記の例でも予測値はどちらも同じような値を示している．
\end{enumerate}
\qed
\end{ans}

\subsection{確率論的解釈}

上記の例では，プロットしたデータを観察して，本質的な構造は線形関係であると判断し仮説関数としては線形関数を選択し，仮説関数の未知のパラメータは，当てはめたときの二乗誤差が小さくなるように，最小二乗誤差関数を選択した．しかし，本質的な構造が線形関係でも，実際のデータは直線上に並んでおらず，ばらつきがある．上記の例ではばらつきについては何も考慮してこなかった．すなわち，$y^{(i)}={\bm \theta}^T{\bm x}$であると決め打ちした上で，データに対していい感じに当てはまるようにパラメータ${\bm \theta}$を定めただけだった．そこで，ここでは，このばらつきもある程度考慮したモデルを考える．

$i$番目のデータに対して，本質的な構造は$y^{(i)}={\bm \theta}^T{\bm x}$とするものの，そこからばらつきが発生しているため，このばらつき項$\varepsilon^{(i)}$を付与して$y^{(i)}={\bm \theta}^T{\bm x}^{(i)}+\varepsilon^{(i)}$というモデルとする．ここで，ばらつき項$\varepsilon^{(i)}$は確率変数であり，$\varepsilon^{(i)}〜N(0,\sigma^2)$に従い$i$について互いに独立かつ同分布(i.i.d:independent and identically distributed)であると仮定する．

\begin{defi}[線形回帰(確率論的)]
トレーニングセット$\{({\bm x}^{(i)},y^{(i)})\}_{i=1}^m \subset (({\cal X}_1 \times {\cal X}_2\times \cdots \times{\cal X}_n) \times {\cal Y})^m$について，${\cal Y}=\mathbb{R}$であり，$y^{(i)}$は式(\ref{LM_p})で生起すると仮定するモデルを{\bf 線形回帰(linear regression)}という．ここで，特徴量${\bm x}^{(i)}$は，常に1の値をとるような特徴量$x_0^{(i)}=1$を付して${\bm x}^{(i)}=(x_0^{(i)},x_1^{(i)},x_2^{(i)},\ldots,x_n^{(i)})^T \in 1 \times {\cal X}_1 \times {\cal X}_2\times \cdots \times{\cal X}_n$と置き直すこととする．また，${\bm \theta}=(\theta_0,\theta_1,\ldots,\theta_n)^T \in \mathbb{R}^{n+1}$とする．また，$\varepsilon^{(i)}$は，$\varepsilon^{(i)}〜N(0,\sigma^2)$に従い$i$について互いに独立かつ同分布(i.i.d)とする．また，$y^{(i)}$は確率変数であることを強調するため$Y^{(i)}$と書くこととする．
\begin{align}
Y^{(i)} = {\bm \theta}^T {\bm x}^{(i)}+\varepsilon^{(i)} \label{LM_p}
\end{align}
\end{defi}

このモデルにより，$Y^{(i)}$は特徴量${\bm x}^{(i)}$とパラメータ${\bm \theta}$が与えられた場合の条件付き確率変数となる．すなわち，$Y^{(i)}|{\bm x}^{(i)},{\bm \theta}$は正規分布に従い生起するというモデルを構築したこととなる．

\begin{rem}
$\varepsilon^{(i)}〜N(0,\sigma^2)$に従い$i$について互いに独立かつ同分布(i.i.d)と仮定しているということと，$Y^{(i)}$の予測に${\bm x}^{(i)}$しか使わないことから，トレーニングセットの各レコードについても互いに独立かつ同分布(i.i.d)を仮定していることに注意する．すなわち，$i$番目のデータ以外は$Y^{(i)}$の予測に全く影響を及ぼさないと仮定している．
\end{rem}

\begin{qu}
式(\ref{LM_p})を仮定した線形回帰において，特徴量${\bm x}^{(i)}$とパラメータ${\bm \theta}$が与えられた場合の条件付き確率変数$y^{(i)}$が従う分布を求めよ．
\end{qu}
\begin{ans}
\begin{align*}
F_{Y^{(i)}|{\bm x}^{(i)},{\bm \theta}}(y^{(i)}|{\bm x}^{(i)},{\bm \theta})&=\mathrm{Pr}\{Y^{(i)} \leq y^{(i)}|{\bm x}^{(i)},{\bm \theta}\}	\\
&=\mathrm{Pr}\{{\bm \theta}^T {\bm x}^{(i)}+\varepsilon^{(i)} \leq y^{(i)}|{\bm x}^{(i)},{\bm \theta}\}	\\
&=\mathrm{Pr}\{\varepsilon^{(i)} \leq y^{(i)}-{\bm \theta}^T {\bm x}^{(i)}|{\bm x}^{(i)},{\bm \theta}\}\\
&=F_{\varepsilon^{(i)}|{\bm x}^{(i)},{\bm \theta}}(y^{(i)}-{\bm \theta}^T {\bm x}^{(i)})
\end{align*}
なので，両辺を$y^{(i)}$で微分すると，
\begin{align*}
\frac{d}{dy^{(i)}}F_{Y^{(i)}|{\bm x}^{(i)},{\bm \theta}}(y^{(i)}|{\bm x}^{(i)},{\bm \theta})&=\frac{d}{dy^{(i)}}F_{\varepsilon^{(i)}|{\bm x}^{(i)},{\bm \theta}}(y^{(i)}-{\bm \theta}^T {\bm x}^{(i)})\\
\Longleftrightarrow 　　　f_{Y^{(i)}|{\bm x}^{(i)},{\bm \theta}}(y^{(i)}|{\bm x}^{(i)},{\bm \theta}) &= f_{\varepsilon^{(i)}|{\bm x}^{(i)},{\bm \theta}}(y^{(i)}-{\bm \theta}^T {\bm x}^{(i)})
\end{align*}
となる．ここで，
\begin{align*}
f_{\varepsilon^{(i)}|{\bm x}^{(i)},{\bm \theta}}(t)=\frac{1}{\sqrt{2\pi }\sigma}e^{-\frac{t^2}{2\sigma^2}}	
\end{align*}
であるから，
\begin{align}
f_{Y^{(i)}|{\bm x}^{(i)},{\bm \theta}}(y^{(i)}|{\bm x}^{(i)},{\bm \theta})=f_{\varepsilon^{(i)}|{\bm x}^{(i)},{\bm \theta}}(y^{(i)}-{\bm \theta}^T {\bm x}^{(i)})=\frac{1}{\sqrt{2\pi }\sigma}e^{-\frac{(y^{(i)}-{\bm \theta}^T {\bm x}^{(i)})^2}{2\sigma^2}}	\label{LR_f}
\end{align}
となる．以上より，$Y^{(i)}|{\bm x}^{(i)},{\bm \theta}$は確率密度関数の形より平均${\bm \theta}^T {\bm x}^{(i)}$，分散$\sigma^2$の正規分布に従う．つまり，$Y^{(i)}|{\bm x}^{(i)},{\bm \theta}〜N({\bm \theta}^T {\bm x}^{(i)},\sigma^2)$．\qed 
\end{ans}

やりたいことは，トレーニングセット$\{({\bm x}^{(i)},y^{(i)})\}_{i=1}^m$から未知のパラメータ${\bm \theta}$を推定することである．これはすなわち単純に分布のパラメータ推定問題であるので，最尤推定で良さげな${\bm \theta}$を求めることができる．最尤推定とは，与えられたトレーニングセット$\{({\bm x}^{(i)},y^{(i)})\}_{i=1}^m$での尤度関数が最大となるように分布のパラメータを定める方法である．尤度関数とは，簡単に言うと全ての$Y^{(i)}$から生起したものが手持ちのデータの通りに生起する確率であり，最尤推定とは手持ちのデータの通りに生起する確率が最も高くなるようにパラメータを調整する作業である．

\begin{defi}[尤度関数，対数尤度関数，最尤推定]
トレーニングセット$\{({\bm x}^{(i)},y^{(i)})\}_{i=1}^m$において，$y^{(i)}$の生起をモデリングしたときの確率変数を$Y^{(i)}|{\bm x}^{(i)},{\bm \theta}$とする(${\bm \theta}$は未知のパラメータ)．このとき，式(\ref{LF1})で表される関数$L({\bm \theta})$を{\bf 尤度関数(likelihood function)}という．特に，$Y^{(i)}$が互いに独立のとき，$L({\bm \theta})$は式(\ref{LF2})となる．
\begin{align}
L({\bm \theta})&=f_{(Y^{(1)},Y^{(2)},\ldots,Y^{(m)})}(y^{(1)},y^{(2)},\ldots,y^{(m)}|{\bm x}^{(1)},{\bm x}^{(2)},\ldots,{\bm x}^{(m)},{\bm \theta})\label{LF1}\\
&=\prod_{i=1}^m f_{Y^{(i)}}(y^{(i)}|{\bm x}^{(i)},{\bm \theta})\label{LF2}
\end{align}
また，未知のパラメータ${\bm \theta}$の決定において，尤度関数を最大とする${\bm \theta}$とする手続きを{\bf 最尤推定(maximum likelihood estimation)}という．なお，尤度関数の最大化の代わりに，式(\ref{LF3})で表される{\bf 対数尤度関数(log likelihood function)}を最大化することが多い．
\begin{align}
\log L({\bm \theta})=\sum_{i=1}^m \log f_{Y^{(i)}}(y^{(i)}|{\bm x}^{(i)},{\bm \theta})\label{LF3}
\end{align}
\end{defi}

さて，これで確率論的な枠組みでの${\bm \theta}$の決定方法の準備が整った．実は，ばらつきのことを考慮しない，天下り的な方法による${\bm \theta}$の決定と最尤推定による${\bm \theta}$の決定は等しい．


\begin{qu}
トレーニングセット$\{({\bm x}^{(i)},y^{(i)})\}_{i=1}^m$に対して式(\ref{LM_p})の線形回帰モデルを構築する．未知のパラメータ${\bm \theta}$を最尤推定により定めることと，式(\ref{LM})の線形回帰モデルにおける最小二乗誤差関数の最小化により定めることが同値であることを示せ．
\end{qu}
\begin{ans}
式(\ref{LM_p})の線形回帰モデルにおいて$Y^{(i)}|{\bm x}^{(i)},{\bm \theta}〜N({\bm \theta}^T {\bm x}^{(i)},\sigma^2)$であるから，確率密度関数は式(\ref{LR_f})の通りである．よって，
\begin{align*}
\log L({\bm \theta})&=\sum_{i=1}^m \log f_{Y^{(i)}}(y^{(i)}|{\bm x}^{(i)},{\bm \theta})\\
&=\sum_{i=1}^m \log \frac{1}{\sqrt{2\pi }\sigma}e^{-\frac{(y^{(i)}-{\bm \theta}^T {\bm x}^{(i)})^2}{2\sigma^2}}\\
&= m \log \frac{1}{\sqrt{2\pi }\sigma}-\frac{1}{2\sigma^2}\sum_{i=1}^m(y^{(i)}-{\bm \theta}^T {\bm x}^{(i)})^2\\
&= m \log \frac{1}{\sqrt{2\pi }\sigma}-\frac{m}{\sigma^2}\frac{1}{2m}\sum_{i=1}^m({\bm \theta}^T {\bm x}^{(i)}-y^{(i)})^2]\\
&=m \log \frac{1}{\sqrt{2\pi }\sigma}-\frac{m}{\sigma^2}J({\bm \theta})
\end{align*}
と式変形できる．ゆえに，$\log L({\bm \theta})$を最大化するためには$J({\bm \theta})$を最小化すると良い．すなわち，未知のパラメータ${\bm \theta}$を最尤推定により定めることと，最小二乗誤差関数の最小化により定めることは同値である．\qed
\end{ans}






