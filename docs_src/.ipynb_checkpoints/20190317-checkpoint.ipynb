{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 教師あり学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレーニングセット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "教師あり学習は，特徴量と目的変数の組のデータを用いて学習する．学習に使用するデータをトレーニングセットという．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [定義] トレーニングセット\n",
    "$\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^m \\subset (({\\cal X}_1 \\times {\\cal X}_2\\times \\cdots \\times{\\cal X}_n) \\times {\\cal Y})^m$を**トレーニングセット(training set)**という．ここで，$m$は**トレーニングサンプル数(number of training examples)**，$\\mathbf{x}^{(i)} \\in {\\cal X}_1 \\times {\\cal X}_2\\times \\cdots \\times {\\cal X}_n$は$i$番目の$n$個の**入力変数(input variables)**または**特徴量(features)**で，$\\mathbf{x}^{(i)}=(x_1^{(i)},x_2^{(i)},\\ldots,x_n^{(i)})^T$と表す．また，$y^{(i)} \\in {\\cal Y}$は$i$番目の**出力変数(output variable)**または**目的変数(target variable)**である．また，トレーニングセットの$i$番目の要素$(\\mathbf{x}^{(i)},y^{(i)})\\in ({\\cal X}_1 \\times {\\cal X}_2\\times \\cdots \\times{\\cal X}_n) \\times {\\cal Y}$を**トレーニングサンプル(training example)**という．また，入力変数のとる空間${\\cal X}_1 \\times {\\cal X}_2\\times \\cdots \\times{\\cal X}_n$を**入力変数空間(space of input values)**，出力変数のとる空間${\\cal Y}$を**出力変数空間(space of output values)**という．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**\n",
    "\n",
    "一般的に(特段の加工をしていない)トレーニングセットには文字列なども含まれるため，特徴量のとりうる集合として数の集合を仮定していないことを明確にするために，単なる集合${\\cal X}_j$と表記している．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [問題]\n",
    "次のトレーニングセットにおいて，$x_3^{(4)}, x_4^{(3)}, y^{(2)}, \\mathbf{x}^{(1)}$を答えよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  $i$  |  $x_1^{(i)}$  |  $x_2^{(i)}$   |  $x_3^{(i)}$  |  $x_4^{(i)}$  | $y^{(i)}$  |\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "|  1  |  2104  |  　5  |  　1  |  apple  |  460  |\n",
    "|  2  |  1416  |  　5  |  　2  |   melon |  232  |\n",
    "|  3  |  1534  |  　3  |  　2  |  lemon  |  315  |\n",
    "|  4  |  852  |  　2  |  　1  |  tomato  |  178  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答)**\n",
    "\n",
    "$x_3^{(4)}=1,~x_4^{(3)}={\\rm lemon},~y^{(2)}=232,~\\mathbf{x}^{(1)}=(2104, 5, 1, {\\rm apple})^T$．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仮説関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "教師あり学習を使って解きたいタスク$T$は，入力変数から出力変数を予測することであるが，それは言い換えると入力変数を引数として出力変数を出力する写像を設定することである．この写像を仮説関数という．仮説関数を以下で定義する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [定義] 仮説関数\n",
    "入力変数$\\mathbf{x}$から出力変数$y$への写像$h:{\\cal X}_1 \\times {\\cal X}_2\\times \\cdots \\times{\\cal X}_n \\to {\\cal Y}$，すなわち$h_{\\mathbf{\\theta}}(\\mathbf{x})$を**仮説関数(hypothesis function)**という．ここで，$\\mathbf{\\theta}$は仮説関数の**パラメータ**である(パラメータは複数あることがほとんどなのでベクトルとしている)．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "すなわち，教師あり学習とは，仮説関数を設定し，トレーニングセットを用いて仮説関数の最適なパラメータを決定することといえる．最適なパラメータを決定できれば，そのパラメータをセットした仮説関数にデータを流し込むことで，そのデータに対する予測値を計算できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 線形回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仮説関数は自らで与える必要があるが，仮説関数の形によって，教師あり学習に特別な名前がつくものがある．例えば，仮説関数を線形関数とし，出力変数空間を${\\cal Y}=\\mathbb{R}$としたときは線形回帰という．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [定義] 線形回帰\n",
    "トレーニングセット$\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^m \\subset (\\mathbb{R}^n \\times {\\cal Y})^m$について，${\\cal Y}=\\mathbb{R}$であり，かつ仮説関数が式(1)である教師あり学習を，特に**線形回帰(linear regression)**という．ここで，特徴量$\\mathbf{x}^{(i)}$は，常に1の値をとるような特徴量$x_0^{(i)}=1$を付して$\\mathbf{x}^{(i)}=(x_0^{(i)},x_1^{(i)},x_2^{(i)},\\ldots,x_n^{(i)})^T \\in 1 \\times \\mathbb{R}^n \\subset \\mathbb{R}^{n+1}$と置き直すこととする．また，$\\mathbf{\\theta}=(\\theta_0,\\theta_1,\\ldots,\\theta_n)^T \\in \\mathbb{R}^{n+1}$とする．\n",
    "$$\n",
    "\\begin{align}\n",
    "h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)}) &= \\theta_0 x_0^{(i)}+\\theta_1 x_1^{(i)} +\\theta_2 x_2^{(i)}+\\cdots + \\theta_n x_n^{(i)} \\nonumber \\\\\n",
    "&= \\sum_{j=0}^n \\theta_j x_j^{(i)}\\nonumber \\\\\n",
    "&= \\mathbf{\\theta}^T \\mathbf{x}^{(i)} \\tag{1}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**\n",
    "\n",
    "仮説関数の形より，線形回帰を用いるためには特徴量を引数にして数値計算を行う必要があるので，トレーニングセット$\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^m \\subset (({\\cal X}_1 \\times {\\cal X}_2\\times \\cdots \\times{\\cal X}_n) \\times {\\cal Y})^m$に文字列が含まれる場合はそのまま適用できず，何らかの規則で数値に変換して$\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^m \\subset (\\mathbb{R}^n \\times \\mathbb{R})^m$の形にしなければならない．このような変換を行う作業を**特徴量エンジニアリング(feature engineerring)**という．特徴量エンジニアリングの話は後で述べる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [問題] \n",
    "ある大学生について，1年次の成績で優をとった個数から2年次にいくつ優をとるのか予測したい．そこで，何人かの大学生の1年次の成績の優の個数$x_1$と2年次の成績の優の個数$y$を集めた．その結果が次表である．このとき，次の問いに答えよ．\n",
    "\n",
    "|  $x_1$  |  $y$  |\n",
    "| ---- | ---- |\n",
    "|  　3  |  　4  |\n",
    "|  　2  |  　1  |\n",
    "|  　4  |  　3  |\n",
    "|  　0  |  　1  |\n",
    "\n",
    "1. $m$はいくつか．\n",
    "1. 仮説関数として$h_{\\mathbf{\\theta}}(\\mathbf{x})=\\theta_0+\\theta_1 x_1$を設定し，本トレーニングセットを用いて線形回帰を行なった結果，パラメータは$\\theta_0=-1,~\\theta_1=2$となった．このとき，1年次の優の個数が6だった大学生の2年次の優の個数を予測せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答)**\n",
    "\n",
    "1. $m=4$．\n",
    "1. $h_{\\mathbf{\\theta}}(\\mathbf{x})=-1+2x_1$なので，$h_{\\mathbf{\\theta}}(6)=-1+2\\cdot 6=11$．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて，仮説関数のパラメータ$\\mathbf{\\theta}$をどう決めるかという問題がある．パラメータをでたらめに与えても良い予測値を返さないので，経験$E$のトレーニングセットを使って，性能指標$P$を高めるように仮説関数のパラメータを更新していくことが必要になる．この手順を学習アルゴリズムといい，性能指標を測る関数を目的関数という．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [定義] 学習アルゴリズム，目的関数\n",
    "性能指標を測る関数$J(\\mathbf{\\theta})$を**目的関数**または**コスト関数(cost function)**といい，この目的関数で測った性能指標が良くなるように仮説関数(のパラメータ$\\mathbf{\\theta}$)を更新していく手順を**学習アルゴリズム(learning algorithm)**という．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回帰問題における性能指標としては，各トレーニングサンプル$(\\mathbf{x}^{(i)},y^{(i)})$における仮説関数$h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})$と$y^{(i)}$の二乗誤差平均が考えられる．この二乗誤差平均を計算する目的関数を平均二乗誤差関数という．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [定義] 平均二乗誤差関数\n",
    "トレーニングセット$\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^m \\subset ((1 \\times \\mathbb{R}^n) \\times \\mathbb{R})^m$について，式(2)で表される目的関数$J(\\mathbf{\\theta})$を**平均二乗誤差関数(mean squared error function)**という．\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\mathbf{\\theta}) = \\frac{1}{2m}\\sum_{i=1}^m (h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})-y^{(i)})^2 \\tag{2}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**\n",
    "\n",
    "$m$ではなく$2m$で割っているのは，微分したときに出てくる2が消えるようにしているからである．$m$でも特段の問題はない(同じ結果が得られる)．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [問題]\n",
    "ある大学生について，1年次の成績で優をとった個数から2年次にいくつ優をとるのか予測したい．そこで，何人かの大学生の1年次の成績の優の個数$x_1$と2年次の成績の優の個数$y$を集めてトレーニングセット(下表)とし，仮説関数として$h_{\\mathbf{\\theta}}(\\mathbf{x})=\\theta_0+\\theta_1 x_1$を設定して線形回帰を行なうことにした．このとき，次の問いに答えよ．結果，パラメータは$\\theta_0=-1,~\\theta_1=2$となった．このとき，トレーニングセットにおける平均二乗誤差$J(\\mathbf{\\theta})$を求めよ．\n",
    "\n",
    "|  $x_1$  |  $y$  |\n",
    "| ---- | ---- |\n",
    "|  　3  |  　4  |\n",
    "|  　2  |  　1  |\n",
    "|  　4  |  　3  |\n",
    "|  　0  |  　1  |\n",
    "\n",
    "1. 平均二乗誤差関数$J(\\mathbf{\\theta})$を書き下し，$\\theta_0,~\\theta_1$の関数として表せ(引数がわかりやすいように$J(\\theta_0, \\theta_1)$と書くことにする)．\n",
    "1. 線形回帰の結果，パラメータは$\\theta_0=-1,~\\theta_1=2$となった．このときの平均二乗誤差関数$J(-1, 2)$の値を求めよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答)**\n",
    "\n",
    "1. 平均二乗誤差関数の定義に，与えられた仮説関数とトレーニングセットを具体的に代入すると，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta_0, \\theta_1) &=\\frac{1}{2\\cdot 4}\\sum_{i=1}^4 (\\theta_0+\\theta_1 x_1^{(i)}-y^{(i)})^2 \\\\\n",
    "&=\\frac{1}{8}\\left((\\theta_0+3\\theta_1-4)^2+(\\theta_0+2\\theta_1-1)^2+(\\theta_0+4\\theta_1-3)^2+(\\theta_0-1)^2\\right) \\\\\n",
    "&=\\frac{1}{8}(4\\theta_0^2+29\\theta_1^2+18\\theta_0\\theta_1-18\\theta_0-52\\theta_1+27)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "2. 実際に$\\theta_0=-1, \\theta_1=2$を代入して計算すると，$J(-1, 2)=\\frac{25}{8}$となる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**\n",
    "\n",
    "学習アルゴリズムにおいては，動かす文字はパラメータ$\\theta$であり，トレーニングセット$\\mathbf{x}^{(i)},y^{(i)}$ではない．トレーニングセットを所与のものとして，目的関数$J(\\theta)$についてパラメータ$\\theta$を色々動かして良くなるような$\\theta$を見つけることが学習アルゴリズムの目的である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最急降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「性能指標が良くなるように仮説関数のパラメータ$\\mathbf{\\theta}$をアップデートしていく」とはどういうことか．目的関数が平均二乗誤差関数の場合，その平均二乗誤差がどんどん小さくなっていくことが，性能指標が良くなっていくといえる．すなわち，目的関数を最小にするパラメータ$\\mathbf{\\theta}$を見つければよい．それを見つけるための手法が学習アルゴリズムである．\n",
    "\n",
    "学習アルゴリズムの中で一般的なものとして最急降下法がある．最急降下法とは，目的関数$J(\\mathbf{\\theta})$のグラフ上に初期値として適当に点を打ち(すなわちパラメータ$\\mathbf{\\theta}$として具体的に何か適当な初期値を決め），その点から周辺を見渡してもっとも勾配が急な方向に一定程度進み，進んだ後の点からまた周辺を見渡してもっとも勾配が急な方向に一定程度進み，，，を繰り返して，どこを見渡しても勾配がないような点を探す方法である．\n",
    "\n",
    "最急降下法を行うためには，関数上のある点について勾配が急な方向はどの方向であるかを計算しなければならない．勾配が最も急な方向を向くベクトルを勾配ベクトルといい，以下で定義される．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [定義] 勾配ベクトル\n",
    "$k$次元ベクトル$\\mathbf{\\theta}=(\\theta_1,\\theta_2,\\ldots,\\theta_k)^T$からスカラー値に写る関数$f(\\mathbf{\\theta})$，すなわち$f:\\mathbf{\\theta} \\in \\mathbb{R}^k \\to \\mathbb{R}$である関数$f(\\mathbf{\\theta})$において，**勾配ベクトル**$\\nabla_{\\mathbf{\\theta}}f=\\frac{\\partial f}{\\partial \\mathbf{\\theta}}\\in \\mathbb{R}^k$は次式で定義される．\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf{\\theta}}f = \\frac{\\partial f}{\\partial \\mathbf{\\theta}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial \\theta_1} \\\\[5pt]\n",
    "\\frac{\\partial f}{\\partial \\theta_2} \\\\[3pt]\n",
    "\\vdots \\\\[5pt]\n",
    "\\frac{\\partial f}{\\partial \\theta_k}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本当に勾配ベクトル方向が，勾配が最も急な方向を指しているのか，2次元を例に示したのが次の問題である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [問題]\n",
    "$f:\\mathbb{R}^2 \\to \\mathbb{R}$の関数$f(x,y)$のある点$P(x,y)$における最大勾配方向が勾配ベクトルの方向と同方向であることを示せ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答)**\n",
    "\n",
    "勾配とは，関数$f$の変化度合いであり，勾配が最大ということは，関数の変化度合いが最も大きいということである．点$P(x,y)$と，そこから微小量$\\Delta x$，$\\Delta y$だけ動かした点$Q(x+\\Delta x,y+\\Delta y)$においてそれぞれ関数値を求めて差をとったものを変化度合い$\\Delta f$とすると，$\\overrightarrow{PQ}=\\overrightarrow{OQ}-\\overrightarrow{OP}=(\\Delta x,\\Delta y)$に注意して，以下の通り変形できる．\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Delta f &= f(x+\\Delta x,y+\\Delta y)-f(x,y) \\\\\n",
    "&= f(x+\\Delta x,y+\\Delta y) -f(x,y+\\Delta y)+f(x,y+\\Delta y) -f(x,y) \\\\\n",
    "&= \\frac{f(x+\\Delta x,y+\\Delta y) -f(x,y+\\Delta y)}{\\Delta x}\\Delta x +\\frac{f(x,y+\\Delta y) -f(x,y)}{\\Delta y}\\Delta y \\\\\n",
    "&\\fallingdotseq \\frac{\\partial f}{\\partial x}\\Delta x +\\frac{\\partial f}{\\partial y}\\Delta y \\\\\n",
    "&= \\nabla f \\cdot (\\Delta x,\\Delta y) \\\\\n",
    "&= \\nabla f \\cdot \\overrightarrow{PQ}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "すなわちこれは，関数の変化度合いは勾配ベクトルと点$P$から点$Q$への方向ベクトル，すなわち微小量を動かした方向のベクトルの内積となっている．ここで，角度の定義より，\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Delta f &= ||\\nabla f||||\\overrightarrow{PQ}||\\cos \\theta\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "となる．ここで，$\\theta$は，$\\nabla f$と$\\overrightarrow{PQ}$のなす角である．関数の変化度合い$\\Delta f$が最大となるのは，$\\cos \\theta =1$，すなわち$\\theta =0$となる場合である．これはつまり$\\nabla f$と$\\overrightarrow{PQ}$が同じ方向を向いているときに関数の変化度合い$\\Delta f$が最大となるということである．以上より，点$P$から関数の変化度合い$\\Delta f$が最大となるように進むためには(点$Q$をとるためには)，勾配ベクトルの方向に進めばよいということである．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで勾配が最も急な方向が勾配ベクトル方向であることがわかったので，それを用いて最急降下法を以下の通り定義する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [定義] 最急降下法\n",
    "関数$J(\\mathbf{\\theta})$を最小とする$\\mathbf{\\theta}$を次の手順で見つけるアルゴリズムを，**最急降下法(gradient descent algorithm)**という．ここで，$\\alpha (>0)$を**学習率(learning rate)**といい，勾配が最大の方向にどの程度移動させるかの強さを表す．\n",
    "\n",
    "1. トレーニングセット$\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^m$，仮説関数$h_{\\mathbf{\\theta}}(\\mathbf{x})$，目的関数$J(\\mathbf{\\theta)}$を用意．\n",
    "2. $\\alpha \\gets $初期値，$\\mathbf{\\theta} \\gets $初期値\n",
    "3. 以下を$\\mathbf{\\theta}$が収束するまでまたは有限回繰り返し\n",
    "    1. $\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^m$を代入して$J(\\mathbf{\\theta)}$を計算\n",
    "    2. $\\nabla_{\\mathbf{\\theta}}J$を計算\n",
    "    3. $\\mathbf{\\theta} \\gets \\mathbf{\\theta}-\\alpha \\nabla_{\\mathbf{\\theta}}J$　(パラメータ$\\theta_0,\\theta_1\\ldots $は同タイミングで更新)\n",
    "4. 得られた$\\mathbf{\\theta}$が$J(\\mathbf{\\theta})$を最小とする$\\mathbf{\\theta}$となる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [問題]\n",
    "$x_0^{(i)}=1$も含め特徴量が$n+1$個のトレーニングセット$\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^m$の線形回帰において，目的関数$J(\\mathbf{\\theta})$を平均二乗誤差関数としたとき，$\\nabla_{\\mathbf{\\theta}}J$を計算せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答)**\n",
    "\n",
    "$\\nabla_{\\mathbf{\\theta}}J$の$j$番目の要素$\\frac{\\partial J}{\\partial \\theta_j}$を計算すると以下となる．\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\theta_j}&= \\frac{1}{2m}\\sum_{i=1}^m \\frac{\\partial }{\\partial \\theta_j}(h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})-y^{(i)})^2 \\\\\n",
    "&=\\frac{1}{2m}\\sum_{i=1}^m 2(h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})-y^{(i)})\\frac{\\partial }{\\partial \\theta_j}h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})\\\\\n",
    "&= \\frac{1}{m}\\sum_{i=1}^m (h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})-y^{(i)})x_j^{(i)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$j=0$のときは$\\frac{\\partial }{\\partial \\theta_0}h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})=1$であることに注意してまとめると，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf{\\theta}}J =\\frac{1}{m}\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=1}^m (h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})-y^{(i)}) \\\\[3pt]\n",
    "\\sum_{i=1}^m (h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})-y^{(i)})x_1^{(i)} \\\\[3pt]\n",
    "\\sum_{i=1}^m (h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})-y^{(i)})x_2^{(i)} \\\\[3pt]\n",
    "\\vdots \\\\[3pt]\n",
    "\\sum_{i=1}^m (h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})-y^{(i)})x_j^{(i)} \\\\[3pt]\n",
    "\\vdots \\\\[3pt]\n",
    "\\sum_{i=1}^m (h_{\\mathbf{\\theta}}(\\mathbf{x}^{(i)})-y^{(i)})x_n^{(i)} \\\\[3pt]\n",
    "\\end{bmatrix}\\tag{3}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "となる（$\\nabla_{\\mathbf{\\theta}}J$は$n+1$次元ベクトルである）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## デザイン行列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最急降下法アルゴリズムは， 式(3)を用いて$\\nabla_{\\mathbf{\\theta}}J$の各要素に対して逐次計算を行っていけば単純に実装できるが，各要素に対して和の計算を行い，反復していくことは少々ややこしい．行列計算を容易に行えるプログラミング言語で実装する場合には，できるだけ逐次計算をしないように，行列計算やベクトル計算を用いて工夫して実装することが簡潔かつバグも少ない．ここでは，デザイン行列を定義し，行列計算により$\\nabla_{\\mathbf{\\theta}}J$を計算できることを示す．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [定義] デザイン行列\n",
    "$x_0^{(i)}=1$も含め特徴量が$n+1$個のトレーニングセット$\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^m\\subset ((1 \\times \\mathbb{R}^n) \\times \\mathbb{R})^m$において，式(4)で定義する行列$X \\in \\mathbb{R}^{m\\times (n+1)}$を**デザイン行列(design matrix)**という．デザイン行列は，特徴量$\\mathbf{x}^{(i)}$を転置してサンプル数の分縦に並べたもので表される．ここで，特徴量として$x_0=1$が加わっていることに注意する．\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "\\mbox{------} & {\\mathbf{x}^{(1)}}^T & \\mbox{------} \\\\\n",
    "\\mbox{------} & {\\mathbf{x}^{(2)}}^T & \\mbox{------} \\\\\n",
    " & \\vdots & \\\\\n",
    "\\mbox{------} & {\\mathbf{x}^{(m)}}^T & \\mbox{------}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \n",
    "\\end{bmatrix} \\tag{4}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [問題]\n",
    "次のトレーニングセットにおいて，デザイン行列$X$を答えよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $i$ | $x_1^{(i)}$ | $x_2^{(i)}$ | $x_3^{(i)}$ | $x_4^{(i)}$ | $y^{(i)}$ |\n",
    "|-----|-----|-----|-----|-----|-----|\n",
    "| 1 | 　2104 | 　5 | 　1 | 　45 | 　460 |\n",
    "| 2 | 　1416 | 　3 | 　2 | 　40 | 　232 |\n",
    "| 3 | 　1534 | 　3 | 　2 | 　30 | 　315 |\n",
    "| 4 | 　852  | 　2 | 　1 | 　36 | 　178 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答)**\n",
    "\n",
    "サンプル数が4つで，特徴量が$x_0=1$を含めると5つなので，$X$は以下のような$4\\times 5$次元の行列となる．\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X=\n",
    "\\begin{bmatrix}\n",
    "1 & 2104 & 5 & 1 & 45 \\\\\n",
    "1 & 1416 & 3 & 2 & 40 \\\\\n",
    "1 & 1534 & 3 & 2 & 30 \\\\\n",
    "1 & 852  & 2 & 1 & 36 \n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "デザイン行列を使うと，予測値である線形回帰の仮説関数のベクトルを簡潔に表すことができ，1発の線形代数計算で予測値を計算できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [問題]\n",
    "$x_0^{(i)}=1$も含め特徴量が$n+1$個のトレーニングセット$\\{(\\mathbf{x}^{(i)},y^{(i)})\\}_{i=1}^m$における線形回帰問題を考える．このとき，$(h_{\\mathbf{\\theta}}(\\mathbf{x}^{(1)}),h_{\\mathbf{\\theta}}(\\mathbf{x}^{(2)}),\\ldots,h_{\\mathbf{\\theta}}(\\mathbf{x}^{(m)}))^T$を$X$，$\\mathbf{\\theta}$を用いて表せ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答)**\n",
    "\n",
    "線形代数の式(7)より，以下のように変形すれば$X$と$\\mathbf{\\theta}$を用いて表すことができる．\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "h_{\\mathbf{\\theta}}(\\mathbf{x}^{(1)}) \\\\\n",
    "h_{\\mathbf{\\theta}}(\\mathbf{x}^{(2)}) \\\\\n",
    "\\vdots \\\\\n",
    "h_{\\mathbf{\\theta}}(\\mathbf{x}^{(m)})\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{\\theta}^T\\mathbf{x}^{(1)} \\\\\n",
    "\\mathbf{\\theta}^T \\mathbf{x}^{(2)}\\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{\\theta}^T \\mathbf{x}^{(m)}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "{\\mathbf{x}^{(1)}}^T\\mathbf{\\theta} \\\\\n",
    "{\\mathbf{x}^{(2)}}^T\\mathbf{\\theta} \\\\\n",
    "\\vdots \\\\\n",
    "{\\mathbf{x}^{(m)}}^T\\mathbf{\\theta}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\mbox{------} & {\\mathbf{x}^{(1)}}^T & \\mbox{------} \\\\\n",
    "\\mbox{------} & {\\mathbf{x}^{(2)}}^T & \\mbox{------} \\\\\n",
    " & \\vdots & \\\\\n",
    "\\mbox{------} & {\\mathbf{x}^{(m)}}^T & \\mbox{------}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "| \\\\[-2pt]\n",
    "| \\\\\n",
    "\\mathbf{\\theta} \\\\\n",
    "| \\\\[-2pt]\n",
    "|\n",
    "\\end{bmatrix}\n",
    "=X \\mathbf{\\theta} \\label{lr_hypo}\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
